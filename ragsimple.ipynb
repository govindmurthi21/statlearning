{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMohU3HTpHAzkNd6vA6UpXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e3a83ff3a9448048e8543119b2547ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79119d6d06a949f380e431a21b0b535d",
              "IPY_MODEL_baba6a49980242fd858666330265a610",
              "IPY_MODEL_6c6544a6850d4f85bf5c9d437dc2488c"
            ],
            "layout": "IPY_MODEL_9f3a87243889412a85cc1af9b21b885c"
          }
        },
        "79119d6d06a949f380e431a21b0b535d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61aeeef7a8c04260856d6ca1de894221",
            "placeholder": "​",
            "style": "IPY_MODEL_41e7a4bc080043e59fe431d790f80a82",
            "value": "100%"
          }
        },
        "baba6a49980242fd858666330265a610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f05eaba0aa404e10a6406b22f15588ee",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f887b533da4488fab1c87c9d2d89a0c",
            "value": 49
          }
        },
        "6c6544a6850d4f85bf5c9d437dc2488c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b408d1da7ac44ebb16e8e3b29e45093",
            "placeholder": "​",
            "style": "IPY_MODEL_d0eae99bd21744e2b5a55fa7afdde37b",
            "value": " 49/49 [02:22&lt;00:00,  2.44s/it]"
          }
        },
        "9f3a87243889412a85cc1af9b21b885c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61aeeef7a8c04260856d6ca1de894221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e7a4bc080043e59fe431d790f80a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f05eaba0aa404e10a6406b22f15588ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f887b533da4488fab1c87c9d2d89a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b408d1da7ac44ebb16e8e3b29e45093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0eae99bd21744e2b5a55fa7afdde37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/govindmurthi21/statlearning/blob/main/ragsimple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0crb9-mUwOp"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.354 \\\n",
        "    openai==1.6.1 \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client==3.1.0 \\\n",
        "    tiktoken==0.5.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata;\n",
        "import os;\n",
        "from langchain.chat_models import ChatOpenAI;\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openapi_key\")\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "6zP6vxynVxHc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
        "]"
      ],
      "metadata": {
        "id": "n06DXB1LWt-X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = chat(messages)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLGYrucPWxEd",
        "outputId": "ec0c1fb2-74af-4811-80c5-13882810f9aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='String theory is a theoretical framework in physics that attempts to reconcile general relativity and quantum mechanics. It posits that the fundamental building blocks of the universe are not point-like particles, but rather tiny, vibrating strings.\\n\\nThese strings can have different vibrational modes, which correspond to different particles with varying properties such as mass and charge. By studying the behavior of these strings, string theory aims to explain the four fundamental forces of nature - gravity, electromagnetism, the weak nuclear force, and the strong nuclear force - in a unified manner.\\n\\nString theory is still a highly speculative and complex area of research, with many unresolved issues and challenges. Researchers continue to explore its implications and potential connections to other areas of physics and cosmology.')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgvcvn5FaOiJ",
        "outputId": "0f932f7e-106b-4875-d7e6-4bb73211488b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String theory is a theoretical framework in physics that attempts to reconcile general relativity and quantum mechanics. It posits that the fundamental building blocks of the universe are not point-like particles, but rather tiny, vibrating strings.\n",
            "\n",
            "These strings can have different vibrational modes, which correspond to different particles with varying properties such as mass and charge. By studying the behavior of these strings, string theory aims to explain the four fundamental forces of nature - gravity, electromagnetism, the weak nuclear force, and the strong nuclear force - in a unified manner.\n",
            "\n",
            "String theory is still a highly speculative and complex area of research, with many unresolved issues and challenges. Researchers continue to explore its implications and potential connections to other areas of physics and cosmology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)\n",
        "prompt = HumanMessage(content=\"Why do physicists believe it can produce a unified theory?\")\n",
        "messages.append(prompt)\n",
        "res = chat(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF35POcYaWux",
        "outputId": "72bdd36a-7612-4f98-b218-be3a1e2a23c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physicists are interested in string theory as a potential candidate for a unified theory because it has the potential to describe all fundamental forces and particles within a single framework. This unified theory, often referred to as a \"theory of everything,\" would provide a consistent description of the universe at both the quantum and cosmic scales.\n",
            "\n",
            "One of the key motivations for pursuing a unified theory is to address the shortcomings of current theories. For example, the Standard Model of particle physics successfully describes three of the four fundamental forces but does not include gravity. General relativity, on the other hand, describes gravity but does not incorporate quantum mechanics.\n",
            "\n",
            "String theory offers a way to potentially unify these theories by incorporating gravity within a quantum framework. By treating particles as vibrating strings, string theory naturally includes gravity along with the other fundamental forces. This has led physicists to investigate whether string theory could provide a consistent and comprehensive description of the universe, leading to the pursuit of a unified theory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)\n",
        "prompt = HumanMessage(content=\"What is so special about Llama 2 and LLMChain in LangChain?\")\n",
        "messages.append(prompt)\n",
        "res = chat(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx1GwHwtb-5A",
        "outputId": "6ff9013e-cc79-4b91-98a8-75634867c182"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I apologize for the confusion, but as of my last update, there is no widely recognized concept or technology called \"Llama 2\" or \"LLMChain in LangChain.\" It is possible that these terms may refer to specific projects, companies, or technologies that are not widely known or recognized.\n",
            "\n",
            "If you can provide more information or context about \"Llama 2\" and \"LLMChain in LangChain,\" I may be able to offer more relevant information or insights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_information = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ],
      "metadata": {
        "id": "vZGl8Wkqcr20"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "Contexts:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\""
      ],
      "metadata": {
        "id": "EcvCeB7McxJm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augmented_prompt\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ],
      "metadata": {
        "id": "A0RAv6Xhcyb3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6ckdmexc4Fi",
        "outputId": "dd188b56-9d4c-4006-ce22-09c2f32a8586"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMChain is a common type of chain within the LangChain framework for developing applications powered by language models. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. The LLMChain takes multiple input variables, formats them into a prompt using the PromptTemplate, passes the prompt to the model, and then utilizes the OutputParser (if provided) to parse the output of the language model into a final format.\n",
            "\n",
            "LangChain is designed to enable powerful and differentiated applications that not only call out to a language model via an API but also incorporate data-awareness and agentic capabilities. This means connecting a language model to other sources of data and allowing the language model to interact with its environment. The LangChain framework is tailored to support these types of applications, providing a flexible and modular approach to leveraging language models for various use cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "db_bfxCVeEts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io_j1NVbeIkz",
        "outputId": "17dbc7c1-da78-4c7b-d919-7635d0214d19"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doi': '1102.0183',\n",
              " 'chunk-id': '0',\n",
              " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
              " 'id': '1102.0183',\n",
              " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
              " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
              " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
              " 'authors': ['Dan C. Cireşan',\n",
              "  'Ueli Meier',\n",
              "  'Jonathan Masci',\n",
              "  'Luca M. Gambardella',\n",
              "  'Jürgen Schmidhuber'],\n",
              " 'categories': ['cs.AI', 'cs.NE'],\n",
              " 'comment': '12 pages, 2 figures, 5 tables',\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.AI',\n",
              " 'published': '20110201',\n",
              " 'updated': '20110201',\n",
              " 'references': []}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = userdata.get(\"pinecone_key\")\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "HJZbCciIf3VG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-west-2\"\n",
        ")"
      ],
      "metadata": {
        "id": "PLlZOsDTgxO8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = 'ragtut1'\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCV93xh_gyUe",
        "outputId": "f98a1333-d09a-460a-98f2-97fc1962390c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXwwBMXiRr3",
        "outputId": "aacc135e-1f62-428f-b70f-e3b2b437374a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed_model.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8c3shgoiT5x",
        "outputId": "6b6caf50-cb30-4212-c179-64701302781e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm  # for progress bar\n",
        "\n",
        "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "    # get text to embed\n",
        "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
        "    # embed text\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['chunk'],\n",
        "         'source': x['source'],\n",
        "         'title': x['title']} for i, x in batch.iterrows()\n",
        "    ]\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8e3a83ff3a9448048e8543119b2547ea",
            "79119d6d06a949f380e431a21b0b535d",
            "baba6a49980242fd858666330265a610",
            "6c6544a6850d4f85bf5c9d437dc2488c",
            "9f3a87243889412a85cc1af9b21b885c",
            "61aeeef7a8c04260856d6ca1de894221",
            "41e7a4bc080043e59fe431d790f80a82",
            "f05eaba0aa404e10a6406b22f15588ee",
            "1f887b533da4488fab1c87c9d2d89a0c",
            "6b408d1da7ac44ebb16e8e3b29e45093",
            "d0eae99bd21744e2b5a55fa7afdde37b"
          ]
        },
        "id": "FMeNIWIuiaKO",
        "outputId": "f6062b9e-558f-491e-a425-b61c1596d320"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/49 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e3a83ff3a9448048e8543119b2547ea"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"  # the metadata field that contains our text\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtliVvJQkq9R",
        "outputId": "12c21c40-e0e8-408c-b3db-da6451a40f92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is so special about Llama 2?\"\n",
        "\n",
        "vectorstore.similarity_search(query, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Y3aetPksSp",
        "outputId": "fde968fd-ca2b-4d8d-91c7-e1672ad07778"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
              " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
              " Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_prompt(query: str):\n",
        "    # get top 3 results from knowledge base\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt"
      ],
      "metadata": {
        "id": "yu2lsKAXkxLm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augment_prompt(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tExbM06k0oY",
        "outputId": "a2262742-dc4d-4133-dde5-84152bb44d79"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the contexts below, answer the query.\n",
            "\n",
            "    Contexts:\n",
            "    \n",
            "\n",
            "    Query: What is so special about Llama 2?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoQpPT5Tk6rP",
        "outputId": "90208148-0eb3-4a67-fceb-2a0a062d3791"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed in the context provided. These LLMs range in scale from 7 billion to 70 billion parameters and are optimized for dialogue use cases. The fine-tuned LLMs within the Llama 2 collection, specifically L/l.sc/a.sc/m.sc/a.sc/two.taboldstyle-C/h.sc/a.sc/t.sc models, outperform open-source chat models on various benchmarks and have demonstrated high performance based on human evaluations for helpfulness and safety.\n",
            "\n",
            "One key aspect that makes Llama 2 special is that these fine-tuned LLMs show promising results compared to closed-source models in terms of performance and usability. The Llama 2 models have been optimized to align with human preferences, enhancing their usability and safety. This level of fine-tuning often requires significant computational resources and human annotation efforts, which are not always transparent or easily reproducible. By providing detailed descriptions of their approach to fine-tuning and safety, the developers of Llama 2 aim to contribute to advancing AI alignment research within the community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"what safety measures were used in the development of llama 2?\"\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60uMNpHUk_SA",
        "outputId": "170561fe-6573-452d-9bba-e4e707d99afb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the development of Llama 2, safety measures were implemented to ensure the models were optimized for dialogue use cases while prioritizing user safety. These safety measures included:\n",
            "\n",
            "1. Fine-tuning for dialogue use cases: The Llama 2 models, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, were specifically fine-tuned for dialogue scenarios, which may require additional considerations for maintaining safety and appropriateness in conversations.\n",
            "\n",
            "2. Human evaluations for helpfulness and safety: The models underwent humane evaluations to assess their effectiveness and safety in interactions with users. This process likely involved gathering feedback from human evaluators to gauge how well the models performed in terms of providing helpful and safe responses.\n",
            "\n",
            "3. Transparency and reproducibility: The development process of the Llama 2 models aimed to be transparent and reproducible, allowing researchers and the community to understand how the models were trained, fine-tuned, and evaluated. This transparency helps ensure accountability and facilitates further research in AI alignment.\n",
            "\n",
            "4. Alignment with human preferences: The closed-product LLMs, which the Llama 2 models were compared against, are heavily fine-tuned to align with human preferences. By considering human preferences, the Llama 2 models aimed to enhance usability and safety, potentially making them suitable substitutes for closed-source models.\n",
            "\n",
            "Overall, the safety measures incorporated into the development of Llama 2 aimed to address concerns related to dialogue interactions, user experience, and ethical considerations when deploying large language models for various applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"what safety measures were used in the development of llama 2?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7WT2WmZlEXx",
        "outputId": "90cad69b-f0bc-4300-ee1a-ce2605f54a1b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the development of Llama 2, several safety measures were implemented to increase the safety of the models. These measures included safety-specific data annotation and tuning, conducting red-teaming exercises, and employing iterative evaluations. \n",
            "\n",
            "Additionally, the researchers shared a thorough description of their fine-tuning methodology and approach to improving the safety of large language models (LLMs). By taking these safety measures and promoting openness in their methodology, the researchers aimed to enable the community to reproduce fine-tuned LLMs and work towards more responsible development of such models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.delete_index(index_name)"
      ],
      "metadata": {
        "id": "eCp4BcjWlJBJ"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}